{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating the binary string model (Hong & Page 2001)\n",
    "\n",
    "Reijula & Kuorikoski (2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating the binary string model, reconstruction based on Hong & Page 2001. \n",
    "\n",
    "Our aim was to examine whether we find the diversity-beats-ability phenomenon in this model (not only in the simplified model described in Hong & Page 2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "\n",
    "#MATPLOTLIB\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spinning_cursor():\n",
    "    while True:\n",
    "        for cursor in '|/-\\\\':\n",
    "            yield cursor\n",
    "\n",
    "sc = spinning_cursor()\n",
    "\n",
    "def update_spinner():\n",
    "    sys.stdout.write('\\b')\n",
    "    sys.stdout.write(next(sc))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "update_spinner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printstring( size , bitstring ):\n",
    "    \"\"\"\n",
    "    return a bit string representation of a bitstring\n",
    "    \"\"\"\n",
    "    bits = size #how many bits to show in the string\n",
    "    return format( bitstring, f'0{bits}b' )\n",
    "    \n",
    "# test output:\n",
    "print( printstring( 4 , 7 ) )\n",
    "print( printstring( 5 , 0b001 ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numDuplicates( array ):\n",
    "    \"\"\"\n",
    "    how many duplicates in an array\n",
    "    \"\"\"\n",
    "    return ( len( array ) - len( np.unique( array ) ) )\n",
    "\n",
    "# tests\n",
    "temp1 = np.array( [1,1,1,3,3,4] )\n",
    "temp2 = np.array( [-1,0,3,4,5] )\n",
    "assert numDuplicates( temp1 ) == 3, \"should be 3\"\n",
    "assert numDuplicates( temp2 ) == 0, \"should be 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipBit( target , mask ):\n",
    "    \"\"\"\n",
    "    xor (target, mask) --> flip bits in target which  are 1s in the mask\n",
    "    \"\"\"\n",
    "    return target ^ mask \n",
    "\n",
    "# tests\n",
    "a = np.random.randint(7)\n",
    "assert flipBit( flipBit(a , a) , a) == a , \"double flip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countOnes( target ):\n",
    "    \"\"\"\n",
    "    return number of 1-bits in a bitstring\n",
    "    \"\"\"\n",
    "    targetStr = format( target, f'0b' )\n",
    "    targetList = [ int(i) for i in list(targetStr) ]\n",
    "    return sum( targetList )\n",
    "\n",
    "countOnesVec = np.vectorize(countOnes) # vectorized version of countOnes, can be applied to numpy arrays\n",
    "\n",
    "# tests\n",
    "testArray = np.array([ 0b001, 0b1111 , 0b101 ])\n",
    "assert (countOnesVec(testArray) == np.array([1 , 4 , 2])).all() , \"ok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_V( size , ls ):\n",
    "    \"\"\"\n",
    "    Returns the value function. \n",
    "    - size: length of bitstring (bits)\n",
    "    - ls: type of problem (simple etc.)\n",
    "    - (C) RANDOM: fully random value function (most difficult possible)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    logger = logging.getLogger( 'init_landscape' )\n",
    "    logger.setLevel( logging.WARNING )\n",
    "    \n",
    "    strings = list( range( 2 ** size ) ) # list of bit positions\n",
    "    \n",
    "    if ls=='random':\n",
    "        s = np.array( strings )\n",
    "        np.random.shuffle( s ) # payoffs = shuffled list of strings ...\n",
    "        payoffs = list( s ) \n",
    "        \n",
    "    # CREATE PANDAS DF WITH SOLUTIONS AS THE INDEX --> FAST INDEXING\n",
    "    df = pd.DataFrame( {'solution':strings , 'payoff':payoffs} )\n",
    "    df = df.set_index( 'solution' )\n",
    "    df.payoff = df.payoff - df.payoff.max() #normalize so that max payoff is 0\n",
    "\n",
    "    logger.debug( f\"String consisting of zeros, payoff: {df.loc[0b0].payoff}\" ) #remember: .iloc accesses by row number\n",
    "    logger.debug( df )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# tests\n",
    "test_df = create_V( 3 , 'random' )\n",
    "min_payoff = test_df.payoff.min()\n",
    "assert np.sum(test_df.index) == np.sum( (test_df.payoff - min_payoff) ) , \"make sure the sums match\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full search\n",
    "\n",
    "def randomString( size ):\n",
    "    return np.random.randint( 2 ** size )\n",
    "\n",
    "def randomMask( size ):\n",
    "    return np.random.randint( 2 ** size )\n",
    "\n",
    "# tests: \n",
    "randString = randomString( 3 )\n",
    "print( f'{randString} aka {format( randString , f\"04b\" )}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P:\n",
    "    MAX_ROUNDS = 1000 # how many trials can the simulation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__( self , size , coversize , fset_set_size , random ):\n",
    "        \"\"\"\n",
    "        returns a new agent: \n",
    "        - size: length of bitstring \n",
    "        - coversize: largest cover of a flipset in the heuristic (see below for implementation detail)\n",
    "        - fset_set_size: how many flipsets in the heuristic \n",
    "        - random: if True, heuristic consists of full-size random heuristics. ignore coversize parameter\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.score = np.zeros( P.MAX_ROUNDS )\n",
    "        self.performance = 0 # expected score from several trials on a landscape\n",
    "        self.size = size # what's the size of the bitstring you're solving\n",
    "        self.fset_set_size = fset_set_size\n",
    "        \n",
    "        flipsets = []\n",
    "        \n",
    "        if random==False:\n",
    "            for i in range( fset_set_size ): \n",
    "                flipsets.append( Agent.generate_flipset( self.size , coversize ) )\n",
    "        else:\n",
    "            for i in range( fset_set_size ):\n",
    "                flipsets.append( randomMask( self.size ) )\n",
    "                \n",
    "        self.set_flipsets( np.array(flipsets) )\n",
    "        \n",
    "        \n",
    "    def generate_flipset( size , coversize ):\n",
    "        \"\"\"\n",
    "        generates  coversize-sized flipsets (set size: fset_set_size) --> flipset heuristic of the agent \n",
    "        \"\"\"\n",
    "        \n",
    "        a = np.arange( start=0 , stop=size , step=1 , dtype=int )\n",
    "        flipped_bit_indices = np.random.choice( a , size=coversize , replace=False ) # draw bit indices WITHOUT replacement\n",
    "\n",
    "        # use indices to switch on bits in the mask\n",
    "        mask = 0\n",
    "        for i in list( flipped_bit_indices ):\n",
    "            mask += 2 ** i\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "        \n",
    "    def set_flipsets( self , flipsets ):\n",
    "        self.flipsets = flipsets\n",
    "    \n",
    "    \n",
    "    def make_group_agent( agent_list ):\n",
    "        \"\"\"\n",
    "        combine flipset heuristics from agents in agent_list into one superagent \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        new_flipsets = np.zeros( (0) , dtype=int ) # store here the flipset set (which grows in size over iterations)\n",
    "        max_cover = 0\n",
    "        \n",
    "        size = agent_list[0].size # length of the bitstring , steal the info from first agent in agent_list\n",
    "        \n",
    "        for a in agent_list: \n",
    "            new_flipsets = np.hstack( (new_flipsets , a.flipsets) ) #merge flipsets\n",
    "        \n",
    "        new_agent = Agent( size , max_cover , len(new_flipsets) , random=False ) # agent w/ dummy flipsets\n",
    "        new_agent.set_flipsets( new_flipsets ) \n",
    "        \n",
    "        new_agent.coversize = np.max( countOnesVec( new_flipsets ) ) # find largest cover size in new flipset-h\n",
    "        \n",
    "        return new_agent\n",
    "\n",
    "    \n",
    "    def print_flipsets( self ):\n",
    "        bitstring_string = [ printstring( self.size , i ) for i in list(self.flipsets) ]\n",
    "        print( bitstring_string )\n",
    "        \n",
    "\n",
    "### TESTS\n",
    "test_flipset = Agent.generate_flipset( size=5 , coversize=4 )\n",
    "print( f'flipset of size, cover 4: {format( test_flipset, f\"05b\" )}\\n' )\n",
    "\n",
    "\n",
    "a = Agent( size=4 , coversize=1 , fset_set_size=3 , random=False ) #remember, params: <size , coversize , fset_set_size , random>\n",
    "b = Agent( size=4 , coversize=2 , fset_set_size=4 , random=False )\n",
    "\n",
    "c = Agent.make_group_agent( [a,b] )\n",
    "\n",
    "print(f'agent a:')\n",
    "a.print_flipsets()\n",
    "\n",
    "print(f'agent b:')\n",
    "b.print_flipsets()\n",
    "\n",
    "print(f'agent c:')\n",
    "c.print_flipsets()\n",
    "\n",
    "print(f'cs coversize: { c.coversize }' )\n",
    "print(f'cs fset_set_size { c.fset_set_size }' )\n",
    "\n",
    "\n",
    "print()\n",
    "f = Agent( size=7, coversize=0 , fset_set_size=4 , random=True ) # here value of coversize is ignored\n",
    "\n",
    "print(f'random mask agent:')\n",
    "f.print_flipsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game( n , value_function , flipsets ):\n",
    "    \"\"\"\n",
    "    run one game:\n",
    "    - n: length of bitstring \n",
    "    - value_function: the used value function \n",
    "    - flipsets: flipset heuristic applied to the problem \n",
    "    \n",
    "    - return a list: (time, payoff, scores)\n",
    "    \n",
    "    - properties (from Hong & Page 2001 whenever applicable):\n",
    "      - application order of flipsets shuffled each time \n",
    "      - search starts from a random string\n",
    "      - switch to a new solution only if strictly higher payoff\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # set up logging for debug\n",
    "    logger = logging.getLogger( 'simu' )\n",
    "    logger.setLevel(logging.WARNING) #set level to DEBUG for testing\n",
    "    \n",
    "    # init\n",
    "    time = 0\n",
    "    fset_index = 0 #index variable for looping through the different flipsets in the set\n",
    "    stop = False\n",
    "    scores = np.zeros( P.MAX_ROUNDS )\n",
    "    \n",
    "    # shuffle flipsets before applying\n",
    "    random_flipsets = np.copy( flipsets ) # make a deep copy of original flipset heuristic so that it doesn't get scrambled\n",
    "    np.random.shuffle( random_flipsets ) # shuffle the application order of flipsets \n",
    "    fset_size = len( random_flipsets )\n",
    "    \n",
    "    # choose a random string as starting position\n",
    "    solution = np.random.choice( value_function.index ) \n",
    "    payoff = value_function.loc[ solution ].payoff\n",
    "    \n",
    "    logger.debug( \"flipsets: \"+ str( [printstring( n , i ) for i in random_flipsets] ) ) \n",
    "    logger.debug( '\\n' )\n",
    "    logger.debug( \"starting position: \"+ printstring( n , solution ) )\n",
    "    logger.debug( \"payoff at starting position: \"+ str( payoff ) )\n",
    "    logger.debug( f'flipset size: {fset_size}' )\n",
    "    logger.debug( 'GO\\n' )\n",
    "    \n",
    "    while not stop: # loop until local/global maximum reached\n",
    "        flipset = random_flipsets[ fset_index ]\n",
    "        scores[ time ] = payoff # store the time series of scores\n",
    "        \n",
    "        # apply the next flipset, switch if better than current\n",
    "        solutionCandidate = flipBit( solution , flipset )\n",
    "        candidatePayoff = value_function.loc[ solutionCandidate ].payoff\n",
    "        \n",
    "        logger.debug( f'time: {time}' )\n",
    "        logger.debug( f'current string: {printstring( n , solution )}')\n",
    "        logger.debug( f'fset_index: {fset_index}' )\n",
    "        logger.debug( f'flipset: {printstring( n , flipset )}' )\n",
    "        logger.debug( f'solutioncandidate: {printstring( n , solutionCandidate )}' )\n",
    "        logger.debug( f'payoff of solution candidate: {candidatePayoff}' )\n",
    "        \n",
    "        if candidatePayoff > payoff:\n",
    "            solution = solutionCandidate\n",
    "            payoff = candidatePayoff\n",
    "            logger.debug( 'switching' )\n",
    "        \n",
    "        # end of round updating\n",
    "        time += 1\n",
    "        fset_index += 1\n",
    "        logger.debug('\\n')\n",
    "        \n",
    "        # if last flipset reached --> loop back to beginning of set of flipsets \n",
    "        if fset_index == fset_size:\n",
    "            fset_index = 0\n",
    "        \n",
    "        # if scores have not improved since the previous round of applying flipsets, stop\n",
    "        if time > fset_size:\n",
    "            if payoff == scores[time-fset_size]:\n",
    "                stop = True\n",
    "\n",
    "    logger.debug( f'Final solution: {printstring( n , solution )}' )\n",
    "    logger.debug( f'Elapsed rounds: {time}' )\n",
    "    \n",
    "    return time, payoff, scores\n",
    "\n",
    "\n",
    "# for testing, set debugger to DEBUG\n",
    "\n",
    "n = 4\n",
    "flipsets_per_agent = 4\n",
    "a = Agent( n , 0 , flipsets_per_agent , random=True )\n",
    "a.print_flipsets()\n",
    "\n",
    "landscape = create_V( n , 'random' )\n",
    "print(landscape)\n",
    "\n",
    "time, payoff, scores = game( a.size , landscape , a.flipsets )\n",
    "\n",
    "a.print_flipsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_performance( agent , v_func , reps ):\n",
    "    \"\"\"\n",
    "    play #reps games with a particular v_func, and a particular (super)agent\n",
    "    return a dict with mean&sd of end_times and end_payoffs\n",
    "    - note: across trials, space remains the same but starting position & application order of flipsets are both random\n",
    "    \"\"\"\n",
    "    \n",
    "    # SET UP LOGGING FOR DEBUG\n",
    "    logger = logging.getLogger( 'calc_perf' )\n",
    "    logger.setLevel(logging.WARNING) #set level to DEBUG for testing\n",
    "    \n",
    "    end_times = np.zeros( reps )\n",
    "    end_payoffs = np.zeros( reps )\n",
    "    \n",
    "    for i in range( reps ):\n",
    "        end_times[i] , end_payoffs[i] , score_timeseries = game( agent.size , v_func , agent.flipsets ) #throwing away the timeseries\n",
    "        logger.debug( f'end time: {end_times[i]}' )\n",
    "        logger.debug( f'end payoff: {end_payoffs[i]}' )                    \n",
    "            \n",
    "    return { \n",
    "            'time_mean':end_times.mean() , \n",
    "            'time_sd':end_times.std() , \n",
    "            'payoff_mean':end_payoffs.mean() , \n",
    "            'payoff_sd':end_payoffs.std() \n",
    "            }\n",
    "\n",
    "\n",
    "# tests\n",
    "\n",
    "n = 8\n",
    "flipsets_per_agent = 4\n",
    "a = Agent( n , 0 , flipsets_per_agent , random=True )\n",
    "landscape = create_V( n , 'random' )\n",
    "\n",
    "results_dict = calc_performance ( a , landscape , 10 )\n",
    "\n",
    "print(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: The no-diminishing-marginal-utility thesis \n",
    "- for context, see Hong & Page 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOCAL PARAMS ###\n",
    "\n",
    "n = 20 #length of bitstring\n",
    "flipsets_per_agent = 3\n",
    "max_group_size = 10\n",
    "reps = 1000 \n",
    "\n",
    "\n",
    "### CREATE AGENTS ###\n",
    "\n",
    "# make a group of individual agents\n",
    "a_list = []\n",
    "for g in range( max_group_size ):\n",
    "    a_list.append( Agent( n , 0 , flipsets_per_agent , random=True ) ) ### 0 is cover size parameter, ignored when random=True\n",
    "\n",
    "# make group-agents of different sizes 1-->\n",
    "groups = []\n",
    "for g in range( max_group_size ):\n",
    "    groups.append( {'size':(g+1) , 'agent':Agent.make_group_agent( a_list[:(g+1)] ) } ) #it's a dict...\n",
    "\n",
    "assert groups[max_group_size-1]['agent'].fset_set_size == max_group_size*flipsets_per_agent # sanity check\n",
    "\n",
    "\n",
    "### SIMULATE ###\n",
    "\n",
    "group_times = []\n",
    "group_times_sd = []\n",
    "\n",
    "group_payoffs = []\n",
    "group_payoffs_sd = []\n",
    "\n",
    "group_sizes = []\n",
    "\n",
    "landscape = create_V( n , 'random' ) # in this exp, all groups play on the same landscape\n",
    "\n",
    "for g in groups:\n",
    "    results = calc_performance( g['agent'] , landscape , reps )\n",
    "    \n",
    "    group_times.append( results['time_mean'] )\n",
    "    group_times_sd.append( results['time_sd'] )\n",
    "    \n",
    "    group_payoffs.append( results['payoff_mean'] )\n",
    "    group_payoffs_sd.append( results['payoff_sd'] ) \n",
    "    \n",
    "    group_sizes.append( g['size'] )\n",
    "\n",
    "\n",
    "group_data = pd.DataFrame( {'group_size':group_sizes , \n",
    "                            'time':group_times , \n",
    "                            'time_sd':group_times_sd, \n",
    "                            'payoff':group_payoffs, \n",
    "                            'payoff_sd':group_payoffs_sd} \n",
    "                         )\n",
    "\n",
    "group_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE\n",
    "\n",
    "fig1  = group_data.plot( legend=True , x='group_size' , \n",
    "                        y='time' , yerr='time_sd' , color='cornflowerblue' ,\n",
    "                        title='Figure 1: Time to local/global optimum solution'\n",
    "                       ).get_figure()\n",
    "\n",
    "fig2 = group_data.plot( legend=True , x='group_size' , \n",
    "                        y='payoff' , yerr='payoff_sd' , color='cornflowerblue' ,\n",
    "                        title='Figure 2: Epistemic value (V) of solution' \n",
    "                      ).get_figure()\n",
    "\n",
    "fig1.get_axes()[0].set_xlabel('group size')\n",
    "fig2.get_axes()[0].set_xlabel('group size')\n",
    "fig1.get_axes()[0].legend(loc='lower right')\n",
    "fig2.get_axes()[0].legend(loc='lower right')\n",
    "\n",
    "fig1.savefig( 'exp1_time_to_solution.png' )\n",
    "fig2.savefig( 'exp1_payoff_at_solution.png' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Does diversity beat ability in the binary string model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOCAL PARAMS ###\n",
    "\n",
    "n = 20 #length of bitstring\n",
    "flipsets_per_agent = 3\n",
    "\n",
    "min_group_size = 1\n",
    "max_group_size = 10\n",
    "\n",
    "reps = 500 \n",
    "tournament_size = 1000\n",
    "\n",
    "v_function_reps = 10 \n",
    "\n",
    "### ### ###\n",
    "\n",
    "# SET UP LOGGING FOR DEBUG\n",
    "logger = logging.getLogger( 'exp2' )\n",
    "logger.setLevel(logging.WARNING) #set level to DEBUG for testing\n",
    "\n",
    "### CREATE AGENTS ###\n",
    "agents = []\n",
    "for g in range( tournament_size ):\n",
    "    agents.append( Agent( n , 999 , flipsets_per_agent , random=True ) ) ### 999 is cover size parameter, ignored when random=True\n",
    "\n",
    "\n",
    "### SIMULATE ###\n",
    "\n",
    "dfs = [] # a list of dataframes\n",
    "    \n",
    "for v in range( v_function_reps ): #repeat the whole thing over different landscapes\n",
    "\n",
    "    landscape = create_V( n , 'random' ) \n",
    "\n",
    "    logger.debug(f'\\nTOURNAMENT {v} \\n')\n",
    "\n",
    "    performance_list = []\n",
    "\n",
    "    for a in agents:\n",
    "        results = calc_performance( a , landscape , reps )\n",
    "        performance_list.append( ( results[ 'payoff_mean' ] , a ) ) # make a list of payoffs and agents\n",
    "        update_spinner()\n",
    "\n",
    "    sorted_list = sorted( performance_list , key=lambda x: x[0] , reverse=True ) # sorted list of agent performance\n",
    "\n",
    "    logger.debug(f'10 best agent scores { [ x[0] for x in sorted_list[:10] ] }')\n",
    "    logger.debug(f'10 worst agent scores { [ x[0] for x in sorted_list[-10:] ] }')\n",
    "    logger.debug(f'10 random agent scores { [ x[0] for x in performance_list[:10] ] }')\n",
    "    \n",
    "    logger.debug(f'\\n10 best agent flipset heuristics:')\n",
    "    for age in [ x[1] for x in sorted_list[:10] ]:\n",
    "        logger.debug(age.flipsets)\n",
    "         \n",
    "    logger.debug(f'\\n10 worst agent flipset heuristics:')\n",
    "    for age in [ x[1] for x in sorted_list[-10:] ]:\n",
    "        logger.debug(age.flipsets)\n",
    "    \n",
    "    logger.debug(f'\\n10 random agent flipset heuristics:')\n",
    "    for age in [ x[1] for x in performance_list[:10] ]:\n",
    "        logger.debug(age.flipsets)\n",
    "    \n",
    "    # RUN WITH DIFFERENT GROUP SIZES\n",
    "    random_group_performance = []\n",
    "    super_group_performance = []\n",
    "\n",
    "    group_size = min_group_size\n",
    "    group_sizes = []\n",
    "\n",
    "    while group_size <= max_group_size:\n",
    "        #logger.debug( f'working on it ... group size {group_size}' )\n",
    "        group_sizes.append(group_size)\n",
    "\n",
    "        # RANDOM GROUP\n",
    "        random_agent_list = performance_list[:group_size] # simply take k first agents, they're not sorted anyway, no bias.  \n",
    "        random_agents = [ x[1] for x in random_agent_list ] # x[1] is the reference to the agent\n",
    "        random_agent_scores = [ x[0] for x in random_agent_list ] \n",
    "        #logger.debug( f'random group agent scores:\\n{ random_agent_scores }\\n' )\n",
    "\n",
    "        random_group = Agent.make_group_agent( random_agents ) # the group of #group_size random agents from the tournament\n",
    "        #logger.debug(f'random group number of duplicate heuristics: { numDuplicates( random_group.flipsets ) }' )\n",
    "        \n",
    "        results = calc_performance( random_group , landscape , reps ) # let the random group play \n",
    "        random_group_performance.append( results['payoff_mean'] )\n",
    "        #logger.debug( f'random group performance: {random_group_performance}\\n' )   \n",
    "\n",
    "        # SUPERGROUP \n",
    "        best_agents = [ x[1] for x in sorted_list[:group_size] ]\n",
    "        best_agent_scores = [ x[0] for x in sorted_list[:group_size] ]\n",
    "        #logger.debug( f'best-performing individual agents:\\n{ best_agent_scores }\\n' )\n",
    "\n",
    "        super_group = Agent.make_group_agent( best_agents ) # the group of #group_size best-performing agents from the tournament\n",
    "        #logger.debug(f'supergroup number of duplicate heuristics: { numDuplicates( super_group.flipsets ) }\\n' )\n",
    "        results = calc_performance( super_group , landscape , reps ) # let the supergroup play \n",
    "        super_group_performance.append( results['payoff_mean'] )\n",
    "        #logger.debug( f'supergroup performance: {super_group_performance}\\n' )\n",
    "\n",
    "        group_size += 1\n",
    "\n",
    "    # SAVE RESULTS:\n",
    "    df = pd.DataFrame( {'group_size':group_sizes , \n",
    "                        'random_group':random_group_performance , \n",
    "                        'supergroup':super_group_performance} \n",
    "                     )\n",
    "\n",
    "    dfs.append(df) # add the dataframe to the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat dataframes and save as csv \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "dateStr = now.strftime(\"d_%Y_%m_%d_h_%H\")\n",
    "\n",
    "concat_frame = pd.concat( dfs )\n",
    "\n",
    "fileName = 'exp2_concatenated_' + dateStr + '.csv' \n",
    "print(fileName)\n",
    "concat_frame.to_csv( path_or_buf=fileName , sep=',', header=True , index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: what happens in the first df\n",
    "\n",
    "dataf = dfs[0]\n",
    "\n",
    "fig1  = dataf.plot( legend=True , x='group_size' , \n",
    "                    y='random_group').get_figure()\n",
    "fig2  = dataf.plot( legend=True , x='group_size' , \n",
    "                    y='supergroup').get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and average over the trials from the different landscapes\n",
    "\n",
    "print( f'reading from file: {fileName}' )\n",
    "new_frame = pd.read_csv(fileName , sep=',' )\n",
    "\n",
    "grouped = new_frame.groupby(['group_size']).mean()\n",
    "\n",
    "grouped['dif'] = grouped.random_group - grouped.supergroup # new col: difference between random and supergroup performance\n",
    "\n",
    "fig1 = grouped['random_group'].plot(legend = True).get_figure()\n",
    "fig2 = grouped['supergroup'].plot(legend = True).get_figure()\n",
    "\n",
    "fig3 = grouped.plot( legend=False, y='dif' , kind='bar' ).get_figure()\n",
    "\n",
    "fig3.get_axes()[0].set_xlabel('Group size')\n",
    "fig3.get_axes()[0].set_ylabel('Random group - High-ability group score')\n",
    "fig3.get_axes()[0].set_ylim((-11000,11000))\n",
    "\n",
    "plotName = dateStr + '_supergroups_vs_random_groups.png' \n",
    "fig3.savefig( plotName )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and average over the trials from the different landscapes\n",
    "\n",
    "# fileName = 'exp2_concatenated_d_2020_10_19_h_12.csv' # remember to update fileName\n",
    "print( f'reading from file: {fileName}' )\n",
    "\n",
    "new_frame = pd.read_csv( fileName , sep=',' )\n",
    "\n",
    "grouped = new_frame.groupby( ['group_size'] , as_index=False ).mean()\n",
    "\n",
    "grouped['dif'] = grouped.supergroup - grouped.random_group # new col: difference between random and supergroup performance\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.bar( grouped['group_size'] , grouped['dif'] )\n",
    "plt.ylim( (-11000,11000) )\n",
    "\n",
    "plt.xlabel(r'Group size')\n",
    "plt.ylabel('Score differential: high-ability - random group')\n",
    "\n",
    "plt.tight_layout() # prevents ylabel from being cut off\n",
    "\n",
    "plotName = 'bitstring_super-random_' + dateStr + '.png'\n",
    "\n",
    "print(plotName)\n",
    "plt.savefig(plotName)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
